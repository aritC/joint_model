{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples,tokenizer,pad_on_right=True,max_length=1024,doc_stride=128,style='combined' ):\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    \n",
    "    tokenized_examples = tokenizer(\n",
    "            examples[\"question\" if pad_on_right else \"context\"],\n",
    "            examples[\"context\" if pad_on_right else \"question\"],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_length,\n",
    "            stride=doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "    \n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    \n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    remove = []\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        \n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        \n",
    "        cls_index = input_ids.index(tokenizer.eos_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "#         answer = examples[\"answers\"][sample_index]\n",
    "        answer = examples['target_text']\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "#         try:\n",
    "        # Start/end character index of the answer in the text.\n",
    "        context = examples['context']\n",
    "        \n",
    "        \n",
    "        start_char = context.lower().find(answer.lower())\n",
    "        if start_char == -1: # not find\n",
    "#             tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "#             tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            remove.append(i)\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        end_char = start_char + len(answer)\n",
    "\n",
    "        # Start token index of the current span in the text.\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "            token_start_index += 1\n",
    "\n",
    "        # End token index of the current span in the text.\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "            token_end_index -= 1\n",
    "        \n",
    "        \n",
    "        # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "            remove.append(i)\n",
    "            \n",
    "        else:\n",
    "            # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "            # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "            while offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "    new_input_ids = []\n",
    "    new_target_ids = []\n",
    "    for i,(ids, attn) in enumerate(zip(tokenized_examples['input_ids'],tokenized_examples['attention_mask'])):\n",
    "        if i not in remove:\n",
    "            new_input_ids.append(ids)\n",
    "            new_target_ids.append(attn)\n",
    "    tokenized_examples['input_ids'] = new_input_ids\n",
    "    tokenized_examples['attention_mask']=new_target_ids\n",
    "    \n",
    "            \n",
    "    return tokenized_examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
